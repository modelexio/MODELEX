{
  "hash": "713c1a886807483518ceb21baafd18a9",
  "result": {
    "markdown": "---\ntitle: \"Classifying newswires: a multi-class classification example\"\nauthor: \" \"\ndate: \"2023-04-27\"\ncategories: [Neural Network, Deep Learning, Reuters dataset, Newswires, NLP, Classification, Keras]\n---\n\n\n\n\n***\n\nThis notebook contains the code samples found in Chapter 3, Section 6 of [Deep Learning with R](https://www.manning.com/books/deep-learning-with-r). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n\n***\n\nIn the previous section we saw how to classify vector inputs into two mutually exclusive classes using a densely-connected neural network. But what happens when you have more than two classes? \n\nIn this section, we will build a network to classify Reuters newswires into 46 different mutually-exclusive topics. Since we have many classes, this problem is an instance of \"multi-class classification\", and since each data point should be classified into only one category, the problem is more specifically an instance of \"single-label, multi-class classification\". If each data point could have belonged to multiple categories (in our case, topics) then we would be facing a \"multi-label, multi-class classification\" problem.\n\n## The Reuters dataset\n\n\nWe will be working with the _Reuters dataset_, a set of short newswires and their topics, published by Reuters in 1986. It's a very simple, widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each topic has at least 10 examples in the training set.\n\nLike IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let's take a look right away:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n\nreuters <- dataset_reuters(num_words = 10000)\nc(c(train_data, train_labels), c(test_data, test_labels)) %<-% reuters\n```\n:::\n\n\nLike with the IMDB dataset, the argument `num_words = 10000` restricts the data to the 10,000 most frequently occurring words found in the data.\n\nWe have 8,982 training examples and 2,246 test examples:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(train_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8982\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(test_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2246\n```\n:::\n:::\n\n\nAs with the IMDB reviews, each example is a list of integers (word indices):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]    1    2    2    8   43   10  447    5   25  207  270    5 3095  111   16\n[16]  369  186   90   67    7   89    5   19  102    6   19  124   15   90   67\n[31]   84   22  482   26    7   48    4   49    8  864   39  209  154    6  151\n[46]    6   83   11   15   22  155   11   15    7   48    9 4579 1005  504    6\n[61]  258    6  272   11   15   22  134   44   11   15   16    8  197 1245   90\n[76]   67   52   29  209   30   32  132    6  109   15   17   12\n```\n:::\n:::\n\n\nHere's how you can decode it back to words, in case you are curious:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nword_index <- dataset_reuters_word_index()\nreverse_word_index <- names(word_index)\nnames(reverse_word_index) <- word_index\ndecoded_newswire <- sapply(train_data[[1]], function(index) {\n  # Note that our indices were offset by 3 because 0, 1, and 2\n  # are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n  word <- if (index >= 3) reverse_word_index[[as.character(index - 3)]]\n  if (!is.null(word)) word else \"?\"\n})\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncat(decoded_newswire)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n```\n:::\n:::\n\n\nThe label associated with an example is an integer between 0 and 45: a topic index.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_labels[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3\n```\n:::\n:::\n\n\n## Preparing the data\n\nWe can vectorize the data with the exact same code as in our previous example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvectorize_sequences <- function(sequences, dimension = 10000) {\n  results <- matrix(0, nrow = length(sequences), ncol = dimension)\n  for (i in 1:length(sequences))\n    results[i, sequences[[i]]] <- 1\n  results\n}\n\nx_train <- vectorize_sequences(train_data)\nx_test <- vectorize_sequences(test_data)\n```\n:::\n\n\nTo vectorize the labels, there are two possibilities: we could just cast the label list as an integer tensor, or we could use a \"one-hot\" encoding. One-hot encoding is a widely used format for categorical data, also called \"categorical encoding\". For a more detailed explanation of one-hot encoding, you can refer to Chapter 6, Section 1. In our case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index, e.g.:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nto_one_hot <- function(labels, dimension = 46) {\n  results <- matrix(0, nrow = length(labels), ncol = dimension)\n  for (i in 1:length(labels))\n    results[i, labels[[i]] + 1] <- 1\n  results\n}\n\none_hot_train_labels <- to_one_hot(train_labels)\none_hot_test_labels <- to_one_hot(test_labels)\n```\n:::\n\n\nNote that there is a built-in way to do this in Keras, which you have already seen in action in our MNIST example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\none_hot_train_labels <- to_categorical(train_labels)\none_hot_test_labels <- to_categorical(test_labels)\n```\n:::\n\n\n## Building our network\n\n\nThis topic classification problem looks very similar to our previous movie review classification problem: in both cases, we are trying to classify short snippets of text. There is however a new constraint here: the number of output classes has gone from 2 to 46, i.e. the dimensionality of the output space is much larger. \n\nIn a stack of dense layers like what we were using, each layer can only access information present in the output of the previous layer. If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each layer can potentially become an \"information bottleneck\". In our previous example, we were using 16-dimensional intermediate layers, but a 16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, permanently dropping relevant information.\n\nFor this reason we will use larger layers. Let's go with 64 units:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>% \n  layer_dense(units = 64, activation = \"relu\", input_shape = c(10000)) %>% \n  layer_dense(units = 64, activation = \"relu\") %>% \n  layer_dense(units = 46, activation = \"softmax\")\n```\n:::\n\n\nThere are two other things you should note about this architecture:\n\n* You end the network with a dense layer of size 46. This means for each input sample, the network will output a 46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n\n* The last layer uses a `softmax` activation. You saw this pattern in the MNIST example. It means the network will  output a _probability distribution_ over the 46 different output classes: that is,  for every input sample, the network will produce a 46-dimensional output vector, where `output[[i]]` is the probability that the sample belongs to class `i`. The 46 scores will sum to 1.\n\n\nThe best loss function to use in this case is `categorical_crossentropy`. It measures the distance between two probability distributions: in our case, between the probability distribution output by our network, and the true distribution of the labels. By minimizing the distance between these two distributions, we train our network to output something as close as possible to the true labels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n```\n:::\n\n\n## Validating our approach\n\nLet's set apart 1,000 samples in our training data to use as a validation set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nval_indices <- 1:1000\n\nx_val <- x_train[val_indices,]\npartial_x_train <- x_train[-val_indices,]\n\ny_val <- one_hot_train_labels[val_indices,]\npartial_y_train = one_hot_train_labels[-val_indices,]\n```\n:::\n\n\nNow let's train our network for 20 epochs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- model %>% fit(\n  partial_x_train,\n  partial_y_train,\n  epochs = 20,\n  batch_size = 512,\n  validation_data = list(x_val, y_val)\n)\n```\n:::\n\n\nLet's display its loss and accuracy curves:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nThe network begins to overfit after nine epochs. Let's train a new network from scratch for nine epochs and then evaluate it on the test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>% \n  layer_dense(units = 64, activation = \"relu\", input_shape = c(10000)) %>% \n  layer_dense(units = 64, activation = \"relu\") %>% \n  layer_dense(units = 46, activation = \"softmax\")\n  \nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nhistory <- model %>% fit(\n  partial_x_train,\n  partial_y_train,\n  epochs = 9,\n  batch_size = 512,\n  validation_data = list(x_val, y_val)\n)\n\nresults <- model %>% evaluate(x_test, one_hot_test_labels)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresults\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     loss  accuracy \n0.9537860 0.7840605 \n```\n:::\n:::\n\n\nOur approach reaches an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier would be 50%, but in our case it is closer to 19%, so our results seem pretty good, at least when compared to a random baseline:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_labels_copy <- test_labels\ntest_labels_copy <- sample(test_labels_copy)\nlength(which(test_labels == test_labels_copy)) / length(test_labels)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1789849\n```\n:::\n:::\n\n\n## Generating predictions on new data\n\nWe can verify that the `predict` method of our model instance returns a probability distribution over all 46 topics. Let's generate topic predictions for all of the test data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions <- model %>% predict(x_test)\n```\n:::\n\n\nEach entry in `predictions` is a vector of length 46:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2246   46\n```\n:::\n:::\n\n\nThe coefficients in this vector sum to 1:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(predictions[1,])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\nThe largest entry is the predicted class, i.e. the class with the highest probability:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwhich.max(predictions[1,])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4\n```\n:::\n:::\n\n\n## A different way to handle the labels and the loss\n\nWe mentioned earlier that another way to encode the labels would be to preserve their integer values. The only thing this approach would change is the choice of the loss function. The previous loss, `categorical_crossentropy`, expects the labels to follow a categorical encoding. With integer labels, you should use `sparse_categorical_crossentropy`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"sparse_categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n```\n:::\n\n\n\nThis new loss function is still mathematically the same as `categorical_crossentropy`; it just has a different interface.\n\n## On the importance of having sufficiently large intermediate layers\n\n\nWe mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden units. Now let's try to see what happens when we introduce an information bottleneck by having intermediate layers significantly less than 46-dimensional, e.g. 4-dimensional.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>% \n  layer_dense(units = 64, activation = \"relu\", input_shape = c(10000)) %>% \n  layer_dense(units = 4, activation = \"relu\") %>% \n  layer_dense(units = 46, activation = \"softmax\")\n  \nmodel %>% compile(\n  optimizer = \"rmsprop\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nmodel %>% fit(\n  partial_x_train,\n  partial_y_train,\n  epochs = 20,\n  batch_size = 128,\n  validation_data = list(x_val, y_val)\n)\n```\n:::\n\n\nOur network now seems to peak at ~71% test accuracy, a 8% absolute drop. This drop is mostly due to the fact that we are now trying to compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is too low-dimensional. The network is able to cram _most_ of the necessary information into these 8-dimensional representations, but not all of it.\n\n## Further experiments\n\n* Try using larger or smaller layers: 32 units, 128 units...\n* We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers.\n\n## Wrapping up\n\n\nHere's what you should take away from this example:\n\n* If you are trying to classify data points between N classes, your network should end with a dense layer of size N.\n* In a single-label, multi-class classification problem, your network should end with a `softmax` activation, so that it will output a probability distribution over the N output classes.\n* _Categorical crossentropy_ is almost always the loss function you should use for such problems. It minimizes the distance between the probability distributions output by the network, and the true distribution of the targets.\n* There are two ways to handle labels in multi-class classification:\n    * Encoding the labels via \"categorical encoding\" (also known as \"one-hot encoding\") and using `categorical_crossentropy` as your loss function.\n    * Encoding the labels as integers and using the `sparse_categorical_crossentropy` loss function.\n* If you need to classify data into a large number of categories, then you should avoid creating information bottlenecks in your network by having intermediate layers that are too small.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}