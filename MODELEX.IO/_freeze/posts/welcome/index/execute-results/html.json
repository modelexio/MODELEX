{
  "hash": "1cf06d2e9bca13f8166b7d6e300d0454",
  "result": {
    "markdown": "---\ntitle: \"Text generation with LSTM\"\nauthor: \"\"\ndate: \"2023-04-24\"\ncategories: [NLP]\n---\n\n\n\n\nThis notebook contains the code samples found in Chapter 8, Section 1 of [Deep Learning with R](https://www.manning.com/books/deep-learning-with-r). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n\n\n## Implementing character-level LSTM text generation\n\n\nLet's put these ideas in practice in a Keras implementation. The first thing we need is a lot of text data that we can use to learn a language model. You could use any sufficiently large text file or set of text files -- Wikipedia, the Lord of the Rings, etc. In this example we will use some of the writings of Nietzsche, the late-19th century German philosopher (translated to English). The language model we will learn will thus be specifically a model of Nietzsche's writing style and topics of choice, rather than a more generic model of the English language.\n\n## Preparing the data\n\nLet's start by downloading the corpus and converting it to lowercase:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\nlibrary(stringr)\n\npath <- get_file(\n  \"nietzsche.txt\",\n  origin = \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n)\ntext <- tolower(readChar(path, file.info(path)$size))\ncat(\"Corpus length:\", nchar(text), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus length: 600893 \n```\n:::\n:::\n\n\nNext, you'll extract partially overlapping sequences of length `maxlen`, one-hot encode them, and pack them in a 3D array `x` of shape `(sequences, maxlen, unique_characters)`. Simultaneously, you'll prepare an array `y` containing the corresponding targets: the one-hot-encoded characters that come after each extracted sequence.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmaxlen <- 60  # Length of extracted character sequences\n\nstep <- 3  # We sample a new sequence every `step` characters\n  \ntext_indexes <- seq(1, nchar(text) - maxlen, by = step)\n\n# This holds our extracted sequences\nsentences <- str_sub(text, text_indexes, text_indexes + maxlen - 1)\n\n# This holds the targets (the follow-up characters)\nnext_chars <- str_sub(text, text_indexes + maxlen, text_indexes + maxlen)\n\ncat(\"Number of sequences: \", length(sentences), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of sequences:  200278 \n```\n:::\n\n```{.r .cell-code}\n# List of unique characters in the corpus\nchars <- unique(sort(strsplit(text, \"\")[[1]]))\ncat(\"Unique characters:\", length(chars), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnique characters: 57 \n```\n:::\n\n```{.r .cell-code}\n# Dictionary mapping unique characters to their index in `chars`\nchar_indices <- 1:length(chars) \nnames(char_indices) <- chars\n\n# Next, one-hot encode the characters into binary arrays.\ncat(\"Vectorization...\\n\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVectorization...\n```\n:::\n\n```{.r .cell-code}\nx <- array(0L, dim = c(length(sentences), maxlen, length(chars)))\ny <- array(0L, dim = c(length(sentences), length(chars)))\nfor (i in 1:length(sentences)) {\n  sentence <- strsplit(sentences[[i]], \"\")[[1]]\n  for (t in 1:length(sentence)) {\n    char <- sentence[[t]]\n    x[i, t, char_indices[[char]]] <- 1\n  }\n  next_char <- next_chars[[i]]\n  y[i, char_indices[[next_char]]] <- 1\n}\n```\n:::\n\n\n## Building the network\n\nThis network is a single LSTM layer followed by a dense classifier and softmax over all possible characters. But note that recurrent neural networks aren't the only way to do sequence data generation; 1D convnets also have proven extremely successful at this task in recent times.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>% \n  layer_lstm(units = 128, input_shape = c(maxlen, length(chars))) %>% \n  layer_dense(units = length(chars), activation = \"softmax\")\n```\n:::\n\n\nSince our targets are one-hot encoded, we will use `categorical_crossentropy` as the loss to train the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimizer <- optimizer_rmsprop(lr = 0.01)\n\nmodel %>% compile(\n  loss = \"categorical_crossentropy\", \n  optimizer = optimizer\n)   \n```\n:::\n\n\n## Training the language model and sampling from it\n\n\nGiven a trained model and a seed text snippet, we generate new text by repeatedly:\n\n* 1) Drawing from the model a probability distribution over the next character given the text available so far\n* 2) Reweighting the distribution to a certain \"temperature\"\n* 3) Sampling the next character at random according to the reweighted distribution\n* 4) Adding the new character at the end of the available text\n\nThis is the code we use to reweight the original probability distribution coming out of the model, and draw a character index from it (the \"sampling function\"):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_next_char <- function(preds, temperature = 1.0) {\n  preds <- as.numeric(preds)\n  preds <- log(preds) / temperature\n  exp_preds <- exp(preds)\n  preds <- exp_preds / sum(exp_preds)\n  which.max(t(rmultinom(1, 1, preds)))\n}\n```\n:::\n\n\nFinally, the following loop repeatedly trains and generates text. You begin generating text using a range of different temperatures after every epoch. This allows you to see how the generated text evolves as the model begins to converge, as well as the impact of temperature in the sampling strategy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (epoch in 1:2) {\n  \n  cat(\"epoch\", epoch, \"\\n\")\n  \n  # Fit the model for 1 epoch on the available training data\n  model %>% fit(x, y, batch_size = 128, epochs = 1) \n  \n  # Select a text seed at random\n  start_index <- sample(1:(nchar(text) - maxlen - 1), 1)  \n  seed_text <- str_sub(text, start_index, start_index + maxlen - 1)\n  \n  cat(\"--- Generating with seed:\", seed_text, \"\\n\\n\")\n  \n  for (temperature in c(0.2, 0.5, 1.0, 1.2)) {\n    \n    cat(\"------ temperature:\", temperature, \"\\n\")\n    cat(seed_text, \"\\n\")\n    \n    generated_text <- seed_text\n    \n     # We generate 20 characters\n    for (i in 1:20) {\n      \n      sampled <- array(0, dim = c(1, maxlen, length(chars)))\n      generated_chars <- strsplit(generated_text, \"\")[[1]]\n      for (t in 1:length(generated_chars)) {\n        char <- generated_chars[[t]]\n        sampled[1, t, char_indices[[char]]] <- 1\n      }\n        \n      preds <- model %>% predict(sampled, verbose = 0)\n      next_index <- sample_next_char(preds[1,], temperature)\n      next_char <- chars[[next_index]]\n      \n      generated_text <- paste0(generated_text, next_char)\n      generated_text <- substring(generated_text, 2)\n      \n      cat(next_char)\n    }\n    cat(\"\\n\\n\")\n  }\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nepoch 1 \n--- Generating with seed: c philosophy must be very much on its guard lest on\naccount  \n\n------ temperature: 0.2 \nc philosophy must be very much on its guard lest on\naccount  \nand and anderes and \n\n------ temperature: 0.5 \nc philosophy must be very much on its guard lest on\naccount  \nin and in thor the b\n\n------ temperature: 1 \nc philosophy must be very much on its guard lest on\naccount  \nfiongopor ingelstron\n\n------ temperature: 1.2 \nc philosophy must be very much on its guard lest on\naccount  \ntf\nrhusy; _ape vo th\n\nepoch 2 \n--- Generating with seed: tep, not the opposite of it.\n\n\n\n\nthe religious life.\n\n\n108\n\n \n\n------ temperature: 0.2 \ntep, not the opposite of it.\n\n\n\n\nthe religious life.\n\n\n108\n\n \nhis in the bed and t\n\n------ temperature: 0.5 \ntep, not the opposite of it.\n\n\n\n\nthe religious life.\n\n\n108\n\n \n a be merse bely the\n\n------ temperature: 1 \ntep, not the opposite of it.\n\n\n\n\nthe religious life.\n\n\n108\n\n \n\" kin the , wo the d\n\n------ temperature: 1.2 \ntep, not the opposite of it.\n\n\n\n\nthe religious life.\n\n\n108\n\n \ncasdtoney of comyeva\n```\n:::\n:::\n\n\n\nHere, we used the random seed text \"new faculty, and the jubilation reached its climax when kant.\" Here's what you get at epoch 20, long before the model has fully converged, with `temperature=0.2`:\n\n```\nnew faculty, and the jubilation reached its climax when kant and such a man\nin the same time the spirit of the surely and the such the such \nas a man is the sunligh and subject the present to the superiority of the \nspecial pain the most man and strange the subjection of the \nspecial conscience the special and nature and such men the subjection of the\nspecial men, the most surely the subjection of the special \nintellect of the subjection of the same things and\n```\n\nHere's the result with `temperature=0.5`:\n\n```\nnew faculty, and the jubilation reached its climax when kant in the eterned \nand such man as it's also become himself the condition of the \nexperience of off the basis the superiory and the special morty of the \nstrength, in the langus, as which the same time life and \"even who \ndiscless the mankind, with a subject and fact all you have to be the stand\nand lave no comes a troveration of the man and surely the \nconscience the superiority, and when one must be w\n```\n\nAnd here's what you get with `temperature=1.0`:\n\n```\nnew faculty, and the jubilation reached its climax when kant, as a \nperiliting of manner to all definites and transpects it it so \nhicable and ont him artiar resull\ntoo such as if ever the proping to makes as cnecience. to been juden, \nall every could coldiciousnike hother aw passife, the plies like \nwhich might thiod was account, indifferent germin, that everythery \ncertain destrution, intellect into the deteriorablen origin of moralian, \nand a lessority o\n```\n\nAt epoch 60, the model has mostly converged, and the text starts to look significantly more coherent. Here's the result with `temperature=0.2`:\n\n```\ncheerfulness, friendliness and kindness of a heart are the sense of the \nspirit is a man with the sense of the sense of the world of the \nself-end and self-concerning the subjection of the strengthorixes--the \nsubjection of the subjection of the subjection of the \nself-concerning the feelings in the superiority in the subjection of the \nsubjection of the spirit isn't to be a man of the sense of the \nsubjection and said to the strength of the sense of the\n```\n\nHere is `temperature=0.5`:\n\n```\ncheerfulness, friendliness and kindness of a heart are the part of the soul\nwho have been the art of the philosophers, and which the one \nwon't say, which is it the higher the and with religion of the frences. \nthe life of the spirit among the most continuess of the \nstrengther of the sense the conscience of men of precisely before enough \npresumption, and can mankind, and something the conceptions, the \nsubjection of the sense and suffering and the\n```\n\nAnd here is `temperature=1.0`:\n\n```\ncheerfulness, friendliness and kindness of a heart are spiritual by the \nciuture for the\nentalled is, he astraged, or errors to our you idstood--and it needs, \nto think by spars to whole the amvives of the newoatly, prefectly \nraals! it was\nname, for example but voludd atu-especity\"--or rank onee, or even all \n\"solett increessic of the world and\nimplussional tragedy experience, transf, or insiderar,--must hast\nif desires of the strubction is be stronges\n```\n\nAs you can see, a low temperature results in extremely repetitive and predictable text, but where local structure is highly realistic: in particular, all words (a word being a local pattern of characters) are real English words. With higher temperatures, the generated text becomes more interesting, surprising, even creative; it may sometimes invent completely new words that sound somewhat plausible (such as \"eterned\" or \"troveration\"). With a high temperature, the local structure starts breaking down and most words look like semi-random strings of characters. Without a doubt, here 0.5 is the most interesting temperature for text generation in this specific setup. Always experiment with multiple sampling strategies! A clever balance between learned structure and randomness is what makes generation interesting.\n\nNote that by training a bigger model, longer, on more data, you can achieve generated samples that will look much more coherent and realistic than ours. But of course, don't expect to ever generate any meaningful text, other than by random chance: all we are doing is sampling data from a statistical model of which characters come after which characters. Language is a communication channel, and there is a distinction between what communications are about, and the statistical structure of the messages in which communications are encoded. To evidence this distinction, here is a thought experiment: what if human language did a better job at compressing communications, much like our computers do with most of our digital communications? Then language would be no less meaningful, yet it would lack any intrinsic statistical structure, thus making it impossible to learn a language model like we just did.\n\n\n## Take aways\n\n* We can generate discrete sequence data by training a model to predict the next tokens(s) given previous tokens.\n* In the case of text, such a model is called a \"language model\" and could be based on either words or characters.\n* Sampling the next token requires balance between adhering to what the model judges likely, and introducing randomness.\n* One way to handle this is the notion of _softmax temperature_. Always experiment with different temperatures to find the \"right\" one.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}